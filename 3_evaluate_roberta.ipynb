{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import parse_args, get_reader, load_model, get_trainer, get_out_filename, write_eval_performance, get_tagset\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.util import wnut_iob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./roberta-train/lightning_logs/version_0/checkpoints/epoch=4-step=4784.ckpt\"\n",
    "checkpoint = \"./roberta-train/lightning_logs/version_0/checkpoints/roberta-train_timestamp_1638429490.282762_final.ckpt\"\n",
    "#checkpoint = \"./roberta-finetune/lightning_logs/version_0/checkpoints/\"\n",
    "checkpoint = \"roberta-finetune/lightning_logs/version_1/checkpoints/finetune_timestamp_1638516609.385282_final.ckpt\"\n",
    "train_file = \"./training_data/EN-English/en_train.conll\"\n",
    "dev_file = \"./training_data/EN-English/en_dev.conll\"\n",
    "output_dir = \"roberta-evaluate\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "iob_tagging = wnut_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 16:12:47 - INFO - reader - Reading file ./training_data/EN-English/en_dev.conll\n",
      "2021-12-03 16:12:48 - INFO - reader - Finished reading 800 instances from file ./training_data/EN-English/en_dev.conll\n",
      "Some weights of the model checkpoint at ./roberta-retrained/checkpoint-5000 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./roberta-retrained/checkpoint-5000 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/malong/opt/anaconda3/envs/sem_eval/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:900: LightningDeprecationWarning: `trainer.test(test_dataloaders)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.test(dataloaders)` instead.\n",
      "  \"`trainer.test(test_dataloaders)` is deprecated in v1.4 and will be removed in v1.6.\"\n",
      "/Users/malong/opt/anaconda3/envs/sem_eval/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 100/100 [01:02<00:00,  1.54it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'ALLPRED': 1241.0,\n",
      " 'ALLRECALLED': 1042.0,\n",
      " 'ALLTRUE': 1230.0,\n",
      " 'F1@CORP': 0.7900552749633789,\n",
      " 'F1@CW': 0.6277777552604675,\n",
      " 'F1@GRP': 0.7823834419250488,\n",
      " 'F1@LOC': 0.8506224155426025,\n",
      " 'F1@PER': 0.9103214740753174,\n",
      " 'F1@PROD': 0.6206896305084229,\n",
      " 'MD@F1': 0.843383252620697,\n",
      " 'MD@P': 0.8396454453468323,\n",
      " 'MD@R': 0.8471544981002808,\n",
      " 'P@CORP': 0.8461538553237915,\n",
      " 'P@CW': 0.614130437374115,\n",
      " 'P@GRP': 0.7704081535339355,\n",
      " 'P@LOC': 0.8266128897666931,\n",
      " 'P@PER': 0.8936877250671387,\n",
      " 'P@PROD': 0.6293706297874451,\n",
      " 'R@CORP': 0.7409326434135437,\n",
      " 'R@CW': 0.6420454382896423,\n",
      " 'R@GRP': 0.7947368621826172,\n",
      " 'R@LOC': 0.8760683536529541,\n",
      " 'R@PER': 0.9275861978530884,\n",
      " 'R@PROD': 0.6122449040412903,\n",
      " 'loss': 5.672953128814697,\n",
      " 'micro@F1': 0.7859166264533997,\n",
      " 'micro@P': 0.7824335098266602,\n",
      " 'micro@R': 0.7894309163093567}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 100/100 [01:02<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 16:13:52 - INFO - util - Finished writing evaluation performance for roberta-evaluate/eval_base_finetune_timestamp_1638516609.385282_final.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = get_reader(file_path=dev_file, target_vocab=wnut_iob, max_instances=-1, max_length=50, encoder_model=\"roberta-base\")\n",
    "model = load_model(checkpoint, tag_to_id=wnut_iob)\n",
    "trainer = get_trainer(is_test=True)\n",
    "out = trainer.test(model, test_dataloaders=DataLoader(test_data, batch_size=8, collate_fn=model.collate_batch))\n",
    "eval_file = get_out_filename(output_dir, checkpoint, prefix=\"eval\")\n",
    "write_eval_performance(out, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() takes exactly one argument (7 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/vyx5gr09397gpv5586rqx5840000gn/T/ipykernel_23242/678120372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpred_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_token_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_stat_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_end_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msub_token1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_token2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_sub_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_token2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() takes exactly one argument (7 given)"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=model.collate_batch)\n",
    "output_result_file = os.path.join(output_dir, \"details_pred.csv\")\n",
    "f = open(output_result_file, \"w\")\n",
    "for batch in test_dataloader:\n",
    "    output = model.perform_forward_step(batch)\n",
    "    pred_result = output[\"pred_results\"]\n",
    "    for i in range(batch_size):\n",
    "        input_ids = batch[0][i]\n",
    "        pred_token_tag = pred_result[i]\n",
    "        metadata_token_tag = batch[3][i]\n",
    "        for (start_pos, end_pos), (pred_stat_pos, pred_end_pos) in zip(metadata_token_tag, pred_token_tag):\n",
    "            sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            pred_sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            tag = metadata_token_tag[(start_pos, end_pos)]\n",
    "            pred_tag = pred_token_tag[(pred_stat_pos, pred_end_pos)]\n",
    "            for sub_token1, sub_token2 in zip(sub_tokens, pred_sub_tokens):\n",
    "                f.write(\"{}{}{}{}{}{}{}\".format(sub_token1, \",\", tag, \",\", sub_token2, \",\", pred_tag))\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31636, 643, 2156, 37, 21, 2243, 9, 5, 2934, 1019, 9, 2174, 1612, 32584, 1635, 8, 919, 9, 5, 579, 2768, 417, 10823, 2070, 1187, 506, 6435, 16687, 1676, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tokenizer(\"among others , he was chairman of the standing conference of regional sports federations and member of the südwestrundfunk broadcasting council\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " 'e',\n",
       " 'ase',\n",
       " 'on',\n",
       " 'down',\n",
       " 'the',\n",
       " 'road',\n",
       " 'âĢĶ',\n",
       " 'ch',\n",
       " 'arl',\n",
       " 'ie',\n",
       " 'sm',\n",
       " 'alls',\n",
       " '(',\n",
       " 'd',\n",
       " 'iana',\n",
       " 'ross',\n",
       " 'and',\n",
       " 'm',\n",
       " 'ichael',\n",
       " 'jack',\n",
       " 'son',\n",
       " 'in',\n",
       " 'the',\n",
       " 'w',\n",
       " 'iz',\n",
       " ')',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tokenizer.convert_ids_to_tokens([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>']\n"
     ]
    }
   ],
   "source": [
    "print(test_data.tokenizer.convert_ids_to_tokens([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26273649c1ad55e65f8b33df1791b797f344b761a59a759c3ddbb97d149ed09b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('sem_eval': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
