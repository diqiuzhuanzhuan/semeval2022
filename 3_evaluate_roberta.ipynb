{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"not in colab\")\n",
    "    pass\n",
    "import os\n",
    "base_dir = \"/content/drive/MyDrive/semeval2022\"\n",
    "if not os.path.exists(base_dir):\n",
    "  !pip install -r requirements.txt\n",
    "  base_dir = \"\"\n",
    "else:\n",
    "  !pip install -r /content/drive/MyDrive/semeval2022/requirements.txt\n",
    "  !cp -rf /content/drive/MyDrive/semeval2022/*.py . \n",
    "  !cp -rf /content/drive/MyDrive/semeval2022/utils .\n",
    "  !cp -rf /content/drive/MyDrive/semeval2022/model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import parse_args, get_reader, load_model, get_trainer, get_out_filename, write_eval_performance, get_tagset\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.util import wnut_iob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = os.path.join(base_dir, \"roberta-train/lightning_logs/version_0/checkpoints/epoch=4-step=4784.ckpt\")\n",
    "checkpoint = os.path.join(base_dir, \"roberta-train/lightning_logs/version_0/checkpoints/roberta-train_timestamp_1638429490.282762_final.ckpt\")\n",
    "#checkpoint = \"./roberta-finetune/lightning_logs/version_0/checkpoints/\"\n",
    "checkpoint = os.path.join(base_dir, \"roberta-finetune/lightning_logs/version_1/checkpoints/finetune_timestamp_1638516609.385282_final.ckpt\")\n",
    "train_file = os.path.join(base_dir, \"training_data/EN-English/en_train.conll\")\n",
    "dev_file = os.path.join(base_dir, \"training_data/EN-English/en_dev.conll\")\n",
    "output_dir = os.path.join(base_dir, \"roberta-evaluate\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "iob_tagging = wnut_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_reader(file_path=dev_file, target_vocab=wnut_iob, max_instances=-1, max_length=55, encoder_model=\"roberta-base\")\n",
    "model = load_model(checkpoint, tag_to_id=wnut_iob)\n",
    "trainer = get_trainer(is_test=True)\n",
    "out = trainer.test(model, test_dataloaders=DataLoader(test_data, batch_size=16, collate_fn=model.collate_batch))\n",
    "eval_file, eval_detail_file = get_out_filename(output_dir, checkpoint, prefix=\"eval\")\n",
    "write_eval_performance(out, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=model.collate_batch)\n",
    "sentences = test_data.sentences\n",
    "ner_tags = test_data.ner_tags\n",
    "pos_to_singel_word_map = test_data.pos_to_single_word_maps\n",
    "f = open(eval_detail_file, \"w\")\n",
    "f.write(\"token\\tlabel\\tpred\\n\")\n",
    "f2 = open(\"en.pred.conll\", \"w\")\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "    output = model.perform_forward_step(batch)\n",
    "    pred_result = output[\"pred_results\"]\n",
    "    raw_pred_results = output[\"raw_pred_results\"]\n",
    "    for i in range(batch_size):\n",
    "        sentence = sentences[idx*batch_size+i]\n",
    "        pos_to_singel_word = pos_to_singel_word_map[idx*batch_size+i]\n",
    "        ner_tag = ner_tags[idx*batch_size+i]\n",
    "        input_ids = batch[0][i]\n",
    "        pred_token_tag = pred_result[i]\n",
    "        raw_pred_token_tag = raw_pred_results[i]\n",
    "        metadata_token_tag = batch[3][i]\n",
    "        meta_labels = []\n",
    "        pred_labels = []\n",
    "        sentence_subtokens = []\n",
    "        for (start_pos, end_pos), (pred_start_pos, pred_end_pos) in zip(metadata_token_tag, pred_token_tag):\n",
    "            sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            sentence_subtokens.extend(sub_tokens)\n",
    "            pred_sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            tag = metadata_token_tag[(start_pos, end_pos)]\n",
    "            pred_tag = pred_token_tag[(pred_start_pos, pred_end_pos)]\n",
    "            for sub_token1, sub_token2 in zip(sub_tokens, pred_sub_tokens):\n",
    "                meta_labels.append(tag)\n",
    "                pred_labels.append(pred_tag)\n",
    "                #f.write(\"{}{}{}{}{}{}{}\".format(sub_token1, \",\", tag, \",\", sub_token2, \",\", pred_tag))\n",
    "                #f.write(\"\\n\")\n",
    "        for (start, end) in pos_to_singel_word:\n",
    "            single_word_tokens = sentence_subtokens[start:end]\n",
    "            word = \"\".join(single_word_tokens)\n",
    "            if word == \"almada\":\n",
    "                print(sentence_subtokens)\n",
    "                print(pos_to_singel_word)\n",
    "            word_meta_tag = ner_tag[start]\n",
    "            word_pred_tag = raw_pred_token_tag[start]\n",
    "            f.write(\"{}\\t{}\\t{}\".format(word, word_meta_tag, word_pred_tag))\n",
    "            f2.write(\"{}\\n\".format(word_pred_tag))\n",
    "            f.write(\"\\n\")\n",
    "        f2.write(\"\\n\")\n",
    "            \n",
    "f.close() \n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_data = pd.read_csv(eval_detail_file, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data = result_data[result_data[\"label\"] != result_data[\"pred\"]]\n",
    "\n",
    "print(len(error_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.tokenizer(\"among others , he was chairman of the standing conference of regional sports federations and member of the s√ºdwestrundfunk broadcasting council\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.sentences[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26273649c1ad55e65f8b33df1791b797f344b761a59a759c3ddbb97d149ed09b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('sem_eval': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
