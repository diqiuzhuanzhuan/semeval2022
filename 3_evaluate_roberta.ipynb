{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import parse_args, get_reader, load_model, get_trainer, get_out_filename, write_eval_performance, get_tagset\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.util import wnut_iob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./roberta-train/lightning_logs/version_0/checkpoints/epoch=4-step=4784.ckpt\"\n",
    "checkpoint = \"./roberta-train/lightning_logs/version_0/checkpoints/roberta-train_timestamp_1638429490.282762_final.ckpt\"\n",
    "#checkpoint = \"./roberta-finetune/lightning_logs/version_0/checkpoints/\"\n",
    "checkpoint = \"roberta-finetune/lightning_logs/version_1/checkpoints/finetune_timestamp_1638516609.385282_final.ckpt\"\n",
    "train_file = \"./training_data/EN-English/en_train.conll\"\n",
    "dev_file = \"./training_data/EN-English/en_dev.conll\"\n",
    "output_dir = \"roberta-evaluate\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "iob_tagging = wnut_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:32:48 - INFO - reader - Reading file ./training_data/EN-English/en_dev.conll\n",
      "2021-12-06 15:32:49 - INFO - reader - Finished reading 800 instances from file ./training_data/EN-English/en_dev.conll\n",
      "Some weights of the model checkpoint at ./roberta-retrained/checkpoint-5000 were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./roberta-retrained/checkpoint-5000 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "test_data = get_reader(file_path=dev_file, target_vocab=wnut_iob, max_instances=-1, max_length=50, encoder_model=\"roberta-base\")\n",
    "model = load_model(checkpoint, tag_to_id=wnut_iob)\n",
    "trainer = get_trainer(is_test=True)\n",
    "#out = trainer.test(model, test_dataloaders=DataLoader(test_data, batch_size=8, collate_fn=model.collate_batch))\n",
    "eval_file, eval_detail_file = get_out_filename(output_dir, checkpoint, prefix=\"eval\")\n",
    "#write_eval_performance(out, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() takes exactly one argument (5 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/vyx5gr09397gpv5586rqx5840000gn/T/ipykernel_6249/2500766444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mword_meta_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mword_pred_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_pred_token_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_meta_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_pred_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() takes exactly one argument (5 given)"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=model.collate_batch)\n",
    "sentences = test_data.sentences\n",
    "ner_tags = test_data.ner_tags\n",
    "pos_to_singel_word_map = test_data.pos_to_single_word_maps\n",
    "f = open(eval_detail_file, \"w\")\n",
    "\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "    output = model.perform_forward_step(batch)\n",
    "    pred_result = output[\"pred_results\"]\n",
    "    raw_pred_results = output[\"raw_pred_results\"]\n",
    "    for i in range(batch_size):\n",
    "        sentence = sentences[idx*batch_size+i]\n",
    "        pos_to_singel_word = pos_to_singel_word_map[idx*batch_size+i]\n",
    "        ner_tag = ner_tags[idx*batch_size+i]\n",
    "        input_ids = batch[0][i]\n",
    "        pred_token_tag = pred_result[i]\n",
    "        raw_pred_token_tag = raw_pred_results[i]\n",
    "        metadata_token_tag = batch[3][i]\n",
    "        meta_labels = []\n",
    "        pred_labels = []\n",
    "        sentence_subtokens = []\n",
    "        for (start_pos, end_pos), (pred_start_pos, pred_end_pos) in zip(metadata_token_tag, pred_token_tag):\n",
    "            sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            sentence_subtokens.extend(sub_tokens)\n",
    "            pred_sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            tag = metadata_token_tag[(start_pos, end_pos)]\n",
    "            pred_tag = pred_token_tag[(pred_start_pos, pred_end_pos)]\n",
    "            for sub_token1, sub_token2 in zip(sub_tokens, pred_sub_tokens):\n",
    "                meta_labels.append(tag)\n",
    "                pred_labels.append(pred_tag)\n",
    "                f.write(\"{}{}{}{}{}{}{}\".format(sub_token1, \",\", tag, \",\", sub_token2, \",\", pred_tag))\n",
    "                f.write(\"\\n\")\n",
    "        for (start, end) in pos_to_singel_word:\n",
    "            single_word_tokens = sentence_subtokens[start:end]\n",
    "            word = \"\".join(single_word_tokens)\n",
    "            word_meta_tag = ner_tag[start]\n",
    "            word_pred_tag = raw_pred_token_tag[start]\n",
    "            f.write(\"{},{},{},\".format(word, word_meta_tag, word_pred_tag))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31636, 643, 2156, 37, 21, 2243, 9, 5, 2934, 1019, 9, 2174, 1612, 32584, 1635, 8, 919, 9, 5, 579, 2768, 417, 10823, 2070, 1187, 506, 6435, 16687, 1676, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tokenizer(\"among others , he was chairman of the standing conference of regional sports federations and member of the südwestrundfunk broadcasting council\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[    1,   242,  3175,   261,  3955,   627, 14288,   578,   611, 11278,\n",
      "           324,  9426, 12019,  1640,   417,  8878, 14500,   463,   119, 25554,\n",
      "         23018,  1478,   179,   627,   605,  1210,    43,     1],\n",
      "        [    1,   405,  7325, 12597, 18160,  1409,  8974, 13447,   179, 45433,\n",
      "           463,   354, 17830,   179, 17265,     4,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,   700, 27905,   560, 12597, 40171,   260,  6493,   417, 23656,\n",
      "            29,     4, 10163,  7820,   179,   627, 15841,  6414,  1116,  1366,\n",
      "          4283,     4,     1,     1,     1,     1,     1,     1],\n",
      "        [    1, 43199,  1258,  1116,   627,   119,  9615, 22760,  1409,   611,\n",
      "          4001,   139,    29, 37837,     4,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,   405, 34188, 30672, 31689, 17723, 18924,   338,   995,   293,\n",
      "          5488, 17341, 17894,   352, 16101,  2989,   338,   995,   293,  1990,\n",
      "           281,  6106,   463, 12782,   876,     4,     1,     1],\n",
      "        [    1, 11404, 11136, 11535, 15129,  7333,  3916,  5225,   438, 45955,\n",
      "             6,  3998,  4822,     6,   463, 12851, 17336,  5069,     4,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,   179, 14517,     6,   405, 12186, 16295,   642,   658,  5290,\n",
      "           261,  9671,   463,  7325,  9471,  8396,  1409,  1116, 16460,     4,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,   700,  7325, 19726,   627,   179,  9399,   368,  1116,   627,\n",
      "          1396,   506, 11107,     4,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]]), tensor([[12,  2,  3,  3,  3,  3,  3, 12,  8,  9,  9,  9,  9, 12,  8,  9,  9, 12,\n",
      "          8,  9,  9,  9, 12,  2,  3,  3, 12, 12],\n",
      "        [12, 12, 12, 12, 12, 12,  0,  1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [12, 12, 12, 12,  4,  5,  5,  5,  8,  9,  9,  9,  9,  9, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [12, 12, 12, 12, 12,  2,  3, 12, 12,  8,  9,  9,  9,  9, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12,  0,  1, 12,  0,  1, 12, 12, 12],\n",
      "        [12,  6,  7,  7,  7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  4,\n",
      "          5, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n",
      "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  0,  1, 12, 12, 12, 12, 12, 12,\n",
      "         12, 12, 12, 12, 12, 12, 12, 12, 12, 12]]), tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "         False, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False]]), ({(0, 0): 'O', (1, 6): 'CW', (7, 7): 'O', (8, 12): 'PER', (13, 13): 'O', (14, 16): 'PER', (17, 17): 'O', (18, 21): 'PER', (22, 22): 'O', (23, 25): 'CW', (26, 26): 'O', (27, 27): 'O'}, {(0, 0): 'O', (1, 1): 'O', (2, 2): 'O', (3, 3): 'O', (4, 4): 'O', (5, 5): 'O', (6, 7): 'CORP', (8, 8): 'O', (9, 9): 'O', (10, 10): 'O', (11, 11): 'O', (12, 12): 'O', (13, 13): 'O', (14, 14): 'O', (15, 15): 'O', (16, 16): 'O'}, {(0, 0): 'O', (1, 1): 'O', (2, 2): 'O', (3, 3): 'O', (4, 7): 'GRP', (8, 13): 'PER', (14, 14): 'O', (15, 15): 'O', (16, 16): 'O', (17, 17): 'O', (18, 18): 'O', (19, 19): 'O', (20, 20): 'O', (21, 21): 'O', (22, 22): 'O'}, {(0, 0): 'O', (1, 1): 'O', (2, 2): 'O', (3, 3): 'O', (4, 4): 'O', (5, 6): 'CW', (7, 7): 'O', (8, 8): 'O', (9, 13): 'PER', (14, 14): 'O', (15, 15): 'O'}, {(0, 0): 'O', (1, 1): 'O', (2, 2): 'O', (3, 3): 'O', (4, 4): 'O', (5, 5): 'O', (6, 6): 'O', (7, 7): 'O', (8, 8): 'O', (9, 9): 'O', (10, 10): 'O', (11, 11): 'O', (12, 12): 'O', (13, 13): 'O', (14, 14): 'O', (15, 15): 'O', (16, 16): 'O', (17, 17): 'O', (18, 18): 'O', (19, 19): 'O', (20, 21): 'CORP', (22, 22): 'O', (23, 24): 'CORP', (25, 25): 'O', (26, 26): 'O'}, {(0, 0): 'O', (1, 4): 'LOC', (5, 5): 'O', (6, 6): 'O', (7, 7): 'O', (8, 8): 'O', (9, 9): 'O', (10, 10): 'O', (11, 11): 'O', (12, 12): 'O', (13, 13): 'O', (14, 14): 'O', (15, 15): 'O', (16, 16): 'O', (17, 17): 'O', (18, 18): 'O', (19, 19): 'O'}, {(0, 0): 'O', (1, 1): 'O', (2, 2): 'O', (3, 3): 'O', (4, 4): 'O', (5, 5): 'O', (6, 6): 'O', (7, 7): 'O', (8, 8): 'O', (9, 9): 'O', (10, 10): 'O', (11, 11): 'O', (12, 12): 'O', (13, 13): 'O', (14, 14): 'O', (15, 15): 'O', (16, 16): 'O', (17, 18): 'GRP', (19, 19): 'O', (20, 20): 'O'}, {(0, 0): 'O', (1, 1): 'O', (2, 2): 'O', (3, 3): 'O', (4, 4): 'O', (5, 5): 'O', (6, 6): 'O', (7, 7): 'O', (8, 8): 'O', (9, 9): 'O', (10, 11): 'CORP', (12, 12): 'O', (13, 13): 'O', (14, 14): 'O'}))\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=model.collate_batch)\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>']\n"
     ]
    }
   ],
   "source": [
    "print(test_data.tokenizer.convert_ids_to_tokens([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'ease',\n",
       " 'on',\n",
       " 'down',\n",
       " 'the',\n",
       " 'road',\n",
       " '—',\n",
       " 'charlie',\n",
       " 'smalls',\n",
       " '(',\n",
       " 'diana',\n",
       " 'ross',\n",
       " 'and',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wiz',\n",
       " ')']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.sentences[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26273649c1ad55e65f8b33df1791b797f344b761a59a759c3ddbb97d149ed09b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('sem_eval': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
