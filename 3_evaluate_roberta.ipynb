{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"not in colab\")\n",
    "    pass\n",
    "import os\n",
    "base_dir = \"/content/drive/MyDrive/semeval2022\"\n",
    "if not os.path.exists(base_dir):\n",
    "  !pip install -r requirements.txt\n",
    "  base_dir = \"\"\n",
    "else:\n",
    "  !pip install -r /content/drive/MyDrive/semeval2022/requirements.txt\n",
    "  !cp -rf /content/drive/MyDrive/semeval2022/*.py . \n",
    "  !cp -rf /content/drive/MyDrive/semeval2022/utils .\n",
    "  !cp -rf /content/drive/MyDrive/semeval2022/model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import parse_args, get_reader, load_model, get_trainer, get_out_filename, write_eval_performance, get_tagset\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.util import wnut_iob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = os.path.join(base_dir, \"roberta-train/lightning_logs/version_0/checkpoints/epoch=4-step=4784.ckpt\")\n",
    "checkpoint = os.path.join(base_dir, \"roberta-train/lightning_logs/version_0/checkpoints/roberta-train_timestamp_1638429490.282762_final.ckpt\")\n",
    "#checkpoint = \"./roberta-finetune/lightning_logs/version_0/checkpoints/\"\n",
    "checkpoint = os.path.join(base_dir, \"roberta-finetune/lightning_logs/version_1/checkpoints/finetune_timestamp_1638516609.385282_final.ckpt\")\n",
    "train_file = os.path.join(base_dir, \"./training_data/EN-English/en_train.conll\")\n",
    "dev_file = os.path.join(base_dir, \"./training_data/EN-English/en_dev.conll\")\n",
    "output_dir = os.path.join(base_dir, \"roberta-evaluate\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "iob_tagging = wnut_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:32:48 - INFO - reader - Reading file ./training_data/EN-English/en_dev.conll\n",
      "2021-12-06 15:32:49 - INFO - reader - Finished reading 800 instances from file ./training_data/EN-English/en_dev.conll\n",
      "Some weights of the model checkpoint at ./roberta-retrained/checkpoint-5000 were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./roberta-retrained/checkpoint-5000 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "test_data = get_reader(file_path=dev_file, target_vocab=wnut_iob, max_instances=-1, max_length=50, encoder_model=\"roberta-base\")\n",
    "model = load_model(checkpoint, tag_to_id=wnut_iob)\n",
    "trainer = get_trainer(is_test=True)\n",
    "#out = trainer.test(model, test_dataloaders=DataLoader(test_data, batch_size=8, collate_fn=model.collate_batch))\n",
    "eval_file, eval_detail_file = get_out_filename(output_dir, checkpoint, prefix=\"eval\")\n",
    "#write_eval_performance(out, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=model.collate_batch)\n",
    "sentences = test_data.sentences\n",
    "ner_tags = test_data.ner_tags\n",
    "pos_to_singel_word_map = test_data.pos_to_single_word_maps\n",
    "f = open(eval_detail_file, \"w\")\n",
    "f.write(\"token\\tlabel\\tpred\\n\")\n",
    "\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "    output = model.perform_forward_step(batch)\n",
    "    pred_result = output[\"pred_results\"]\n",
    "    raw_pred_results = output[\"raw_pred_results\"]\n",
    "    for i in range(batch_size):\n",
    "        sentence = sentences[idx*batch_size+i]\n",
    "        pos_to_singel_word = pos_to_singel_word_map[idx*batch_size+i]\n",
    "        ner_tag = ner_tags[idx*batch_size+i]\n",
    "        input_ids = batch[0][i]\n",
    "        pred_token_tag = pred_result[i]\n",
    "        raw_pred_token_tag = raw_pred_results[i]\n",
    "        metadata_token_tag = batch[3][i]\n",
    "        meta_labels = []\n",
    "        pred_labels = []\n",
    "        sentence_subtokens = []\n",
    "        for (start_pos, end_pos), (pred_start_pos, pred_end_pos) in zip(metadata_token_tag, pred_token_tag):\n",
    "            sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            sentence_subtokens.extend(sub_tokens)\n",
    "            pred_sub_tokens = test_data.tokenizer.convert_ids_to_tokens(input_ids[start_pos: end_pos+1])\n",
    "            tag = metadata_token_tag[(start_pos, end_pos)]\n",
    "            pred_tag = pred_token_tag[(pred_start_pos, pred_end_pos)]\n",
    "            for sub_token1, sub_token2 in zip(sub_tokens, pred_sub_tokens):\n",
    "                meta_labels.append(tag)\n",
    "                pred_labels.append(pred_tag)\n",
    "                #f.write(\"{}{}{}{}{}{}{}\".format(sub_token1, \",\", tag, \",\", sub_token2, \",\", pred_tag))\n",
    "                #f.write(\"\\n\")\n",
    "        for (start, end) in pos_to_singel_word:\n",
    "            single_word_tokens = sentence_subtokens[start:end]\n",
    "            word = \"\".join(single_word_tokens)\n",
    "            word_meta_tag = ner_tag[start]\n",
    "            word_pred_tag = raw_pred_token_tag[start]\n",
    "            f.write(\"{}\\t{}\\t{}\".format(word, word_meta_tag, word_pred_tag))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_data = pd.read_csv(eval_detail_file, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            token   label    pred\n",
      "15            the    B-CW       O\n",
      "16            wiz    I-CW       O\n",
      "76         marble       O   B-LOC\n",
      "77         canyon       O   I-LOC\n",
      "79          cliff       O   B-LOC\n",
      "...           ...     ...     ...\n",
      "13411      kyselo       O  B-PROD\n",
      "13413   krkonoÅ¡e       O    B-CW\n",
      "13419      boiled  B-PROD       O\n",
      "13420         egg  I-PROD       O\n",
      "13437  vaudeville    B-CW  B-CORP\n",
      "\n",
      "[630 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "error_data = result_data[result_data[\"label\"] != result_data[\"pred\"]]\n",
    "print(error_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31636, 643, 2156, 37, 21, 2243, 9, 5, 2934, 1019, 9, 2174, 1612, 32584, 1635, 8, 919, 9, 5, 579, 2768, 417, 10823, 2070, 1187, 506, 6435, 16687, 1676, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tokenizer(\"among others , he was chairman of the standing conference of regional sports federations and member of the südwestrundfunk broadcasting council\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'ease',\n",
       " 'on',\n",
       " 'down',\n",
       " 'the',\n",
       " 'road',\n",
       " '—',\n",
       " 'charlie',\n",
       " 'smalls',\n",
       " '(',\n",
       " 'diana',\n",
       " 'ross',\n",
       " 'and',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wiz',\n",
       " ')']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.sentences[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26273649c1ad55e65f8b33df1791b797f344b761a59a759c3ddbb97d149ed09b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('sem_eval': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
