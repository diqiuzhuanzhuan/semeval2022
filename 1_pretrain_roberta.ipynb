{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ModuleNotFoundError as e:\n",
        "    print(\"not in colab\")\n",
        "    pass\n",
        "import os\n",
        "base_dir = \"/content/drive/MyDrive/semeval2022\"\n",
        "if not os.path.exists(base_dir):\n",
        "  !pip install -r requirements.txt\n",
        "  base_dir = \"\"\n",
        "else:\n",
        "  !pip install -r /content/drive/MyDrive/semeval2022/requirements.txt\n",
        "  !cp -rf /content/drive/MyDrive/semeval2022/*.py . \n",
        "  !cp -rf /content/drive/MyDrive/semeval2022/utils .\n",
        "  !cp -rf /content/drive/MyDrive/semeval2022/model ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "fOPX9JoyaPcY",
        "outputId": "5c6fe3f4-5332-47ba-faa6-31860ea8f815"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "from utils.util import wnut_iob\n",
        "import os\n",
        "from utils.util import get_reader, train_model, create_model, save_model, parse_args, get_tagset\n",
        "train_file = os.path.join(base_dir, \"training_data/EN-English/en_train.conll\")\n",
        "dev_file = os.path.join(base_dir, \"training_data/EN-English/en_dev.conll\")\n",
        "encoder_model = \"bert-base-uncased\"\n",
        "train_reader = get_reader(file_path=train_file, target_vocab=get_tagset(wnut_iob), encoder_model=encoder_model)\n",
        "dev_reader = get_reader(file_path=dev_file, target_vocab=get_tagset(wnut_iob), encoder_model=encoder_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLWh1pPfaPcZ",
        "outputId": "c4fa7ccd-d2eb-40cf-e611-6b3e74dc84be"
      },
      "outputs": [],
      "source": [
        "print(train_reader.sentences.__len__())\n",
        "print(dev_reader.__len__())\n",
        "from typing import List\n",
        "pretrain_txt = os.path.join(base_dir, \"pretrain.txt\")\n",
        "def write_pretrain_sentences(data: List[List], filename: str):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for sentences in data:\n",
        "            for sentence in sentences:\n",
        "                f.write(sentence)\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "write_pretrain_sentences([train_reader.sentences, dev_reader.sentences], pretrain_txt)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJdr9DZYaPcZ",
        "outputId": "19fdb717-f0a5-47f5-9017-9af696b0dc9a"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModelForPreTraining\n",
        "from transformers import LineByLineTextDataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "output_dir = os.path.join(base_dir, \"self_pretrain\", encoder_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(encoder_model)\n",
        "model = AutoModelForPreTraining.from_pretrained(encoder_model)\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=pretrain_txt,\n",
        "    block_size=512,\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTDtwyRxaPca"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_pretrain_roberta.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "26273649c1ad55e65f8b33df1791b797f344b761a59a759c3ddbb97d149ed09b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('sem_eval': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
